---
title: "Assignment 3"
author: "Arthur Junges Schmidt"
date: "`r as.character(Sys.time(), '%d %B %Y')`"
documentclass: article
lang: eng
classoption: a4paper
output:
  pdf_document:
    fig_width: 10
    fig_height: 6
    
geometry: margin=2cm
bibliography: references.bib
biblio-style: "apalike"
link-citations: true

---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(e1071)
library(beepr)
library(caret)
library(randomForest)
library(ranger)
```

# Problem 1
(a) The file ps3-1.csv contains a data set with 34 features (x1, x2, â€¦ x34) and 1 target variable
(Y). Estimate a classifier model using Support Vector Machine and Random Forest algorithm in
R, respectively, with the first 14628 rows of the data. Optimize your model so that a False
Positive Rate is less than 10% for Y = 0 (actual Y = 1 cases falsely classified as Y = 0).
Particularly, you are required to review relevant literature, use the k-fold cross-validation method
to train the Random Forest model. Use grid search to find hyper-parameter setting: the best
number of trees and features, maximum leaf nodes. Assess importance of each feature based on
two criteria: Mean Decrease Accuracy and Mean Decrease Gini.
Compare two estimation methods with confusion matrices

```{r Data reading, message=FALSE}
Crude_Data <- read.csv("ps3-1.csv");
Crude_Data <- Crude_Data[,-1];
Crude_Data$Y <- factor(Crude_Data$Y, levels = c(0,1));
Training_Data <- Crude_Data[1:14628, ];
Test_Data <- Crude_Data[-(1:14628), ];
```


```{r SVM, cache=TRUE}
# SVM_Model <- svm(Y ~ ., data = Training_Data)
# #plot(SVM_Model, data = Training_Data, Y~.)
# SVM_Predict <- predict(object = SVM_Model, newdata = Test_Data[,-35]);
# table(Predicted = Test_Data[,35], True = SVM_Predict)
# test <- tune.svm(Y ~ ., data = Training_Data, gamma = c(0.01, 0.1, 1), cost = 5, kernel = "radial")
# beepr::beep(sound = 9)

```

# Random Forest

According to @friedman2001elements and @james2013introduction, the usual number of predictors candidates $m$ from the full set of predictors $p$ is $$m  \approx  \sqrt{p}$$. So with our data, $m \approx \sqrt(35) \approx 6$.
```{r Random Forest, cache=TRUE}
tunegrid <- expand.grid(.mtry = c(15), .splitrule = c("gini", "extratrees"),
                      .min.node.size = c(5))
fitControl <- trainControl(method = "cv", number = 5, search = "grid", verboseIter = TRUE)
modellist <- list()
Start_Time <- Sys.time()
for (ntree in c(1000, 2000)) {
	set.seed(42)
  cat(ntree)
	fit <- train(Y ~ ., data = Training_Data,
                             method = "ranger", trControl = fitControl,
                             tuneGrid = tunegrid, num.trees = ntree,  
                             )
	key <- toString(ntree)
	modellist[[key]] <- fit
	running_time <- Sys.time() - Start_Time
}
# Random_Forest_Model <- train(Y ~ ., data = Training_Data,
#                              method = "rf", trControl = fitControl,
#                              tuneGrid = tunegrid
#                              )

# compare results
results <- resamples(modellist)
summary(results)
dotplot(results)
beepr::beep(sound = 9)
```

